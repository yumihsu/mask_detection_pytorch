{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "source": [
    "# load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 因為xml檔案讀取後是 object ，所以需要用 beautifulSoup 套件轉成 text\n",
    "### 需要轉換的項目有 boxes , label\n",
    "### 最後將其輸出成已經是 tensor 的 dic\n",
    "### The dtype of labels in detection model shoud be int64  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(obj):\n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    return [xmin,ymin,xmax,ymax]\n",
    "\n",
    "def get_label(obj):\n",
    "    if obj.find('name').text =='with_mask' :\n",
    "        return 1 \n",
    "    elif obj.find('name').text =='without_mask':\n",
    "        return 2 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_to_target(image_id , file):\n",
    "    with open (file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data,'xml')\n",
    "        objects = soup.find_all('object')\n",
    "        obj_len = len(objects)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(get_box(i))\n",
    "            labels.append(get_label(i))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes,dtype = torch.float32)\n",
    "        \n",
    "        labels = torch.as_tensor(labels,dtype = torch.int64) \n",
    "        image_id = torch.as_tensor([image_id],dtype = torch.float32)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = list(sorted(os.listdir(\"images/\")))\n",
    "labels = list(sorted(os.listdir(\"annotations/\")))"
   ]
  },
  {
   "source": [
    "### Create DataSet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self,trasforms):\n",
    "        self.trasforms = trasforms\n",
    "        self.imgs = list(sorted(os.listdir(\"images/\")))\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file_image = \"maksssksksss\" + str(idx) + \".png\"\n",
    "        file_labels = \"maksssksksss\" + str(idx) + \".xml\"\n",
    "        image_path = os.path.join(\"images/\",file_image)\n",
    "        label_path = os.path.join(\"annotations/\",file_labels)\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        target = obj_to_target(idx, label_path)\n",
    "\n",
    "        if self.trasforms != None:\n",
    "            img = self.trasforms(img)\n",
    "        return img , target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "def collate_size(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskDataset(data_transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,batch_size=4,collate_fn=collate_size)"
   ]
  },
  {
   "source": [
    "# model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num):\n",
    "    # get pretrain model \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained= True)\n",
    "    # set new feature layers\n",
    "    in_feature = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the new layer with parameter\n",
    "    model.roi_heads.box_predictor  = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_feature, num_classes=num)\n",
    "    return model\n",
    "\n",
    "    \n",
    "# none - withmask - withoutmask\n",
    "model = get_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCHS:0  LOSS:75.2225341796875\n",
      "EPOCHS:1  LOSS:62.20501708984375\n",
      "EPOCHS:2  LOSS:52.863094329833984\n",
      "EPOCHS:3  LOSS:42.18647766113281\n",
      "EPOCHS:4  LOSS:39.41168975830078\n",
      "EPOCHS:5  LOSS:36.258853912353516\n",
      "EPOCHS:6  LOSS:34.88839340209961\n",
      "EPOCHS:7  LOSS:34.32380676269531\n",
      "EPOCHS:8  LOSS:34.67509460449219\n",
      "EPOCHS:9  LOSS:30.84820556640625\n",
      "EPOCHS:10  LOSS:29.999479293823242\n",
      "EPOCHS:11  LOSS:28.575380325317383\n",
      "EPOCHS:12  LOSS:28.038957595825195\n",
      "EPOCHS:13  LOSS:28.956459045410156\n",
      "EPOCHS:14  LOSS:27.612422943115234\n",
      "EPOCHS:15  LOSS:25.183340072631836\n",
      "EPOCHS:16  LOSS:26.732450485229492\n",
      "EPOCHS:17  LOSS:25.95669937133789\n",
      "EPOCHS:18  LOSS:23.627683639526367\n",
      "EPOCHS:19  LOSS:22.590290069580078\n",
      "EPOCHS:20  LOSS:24.676746368408203\n",
      "EPOCHS:21  LOSS:20.787858963012695\n",
      "EPOCHS:22  LOSS:20.085939407348633\n",
      "EPOCHS:23  LOSS:22.406620025634766\n",
      "EPOCHS:24  LOSS:22.4732723236084\n",
      "EPOCHS:25  LOSS:23.509784698486328\n",
      "EPOCHS:26  LOSS:23.369844436645508\n",
      "EPOCHS:27  LOSS:19.5292911529541\n",
      "EPOCHS:28  LOSS:19.734357833862305\n",
      "EPOCHS:29  LOSS:21.348487854003906\n",
      "EPOCHS:30  LOSS:21.060317993164062\n",
      "EPOCHS:31  LOSS:20.247783660888672\n",
      "EPOCHS:32  LOSS:16.908546447753906\n",
      "EPOCHS:33  LOSS:16.912813186645508\n",
      "EPOCHS:34  LOSS:18.697589874267578\n",
      "EPOCHS:35  LOSS:18.08333396911621\n",
      "EPOCHS:36  LOSS:18.03433609008789\n",
      "EPOCHS:37  LOSS:18.293102264404297\n",
      "EPOCHS:38  LOSS:19.381023406982422\n",
      "EPOCHS:39  LOSS:19.467504501342773\n",
      "EPOCHS:40  LOSS:18.315507888793945\n",
      "EPOCHS:41  LOSS:20.424776077270508\n",
      "EPOCHS:42  LOSS:19.067956924438477\n",
      "EPOCHS:43  LOSS:19.978721618652344\n",
      "EPOCHS:44  LOSS:17.77311897277832\n",
      "EPOCHS:45  LOSS:16.840099334716797\n",
      "EPOCHS:46  LOSS:18.20623016357422\n",
      "EPOCHS:47  LOSS:16.966032028198242\n",
      "EPOCHS:48  LOSS:17.594728469848633\n",
      "EPOCHS:49  LOSS:16.129905700683594\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params,lr= 5e-3 , momentum=9e-1 , weight_decay= 5e-4)\n",
    "len_loader = len(data_loader)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    i = 0 \n",
    "    for imgs , annotations in data_loader:\n",
    "        i+=1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k,v in t.items()} for t in annotations]\n",
    "\n",
    "        loss_dict = model([imgs[0]],[annotations[0]])\n",
    "        losses = sum(loss for loss  in  loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses\n",
    "        #print(losses)\n",
    "    print(\"EPOCHS:{}  LOSS:{}\".format(epoch,epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"mdoel_0720.pth\")\n",
    "torch.save(model.state_dict(),\"mdoel_state_0720.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs , annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k,v in t.items()} for t in annotations]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}